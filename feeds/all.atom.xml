<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Paper Survey</title><link href="https://matasukef.github.io/papers/" rel="alternate"></link><link href="https://matasukef.github.io/papers/feeds/all.atom.xml" rel="self"></link><id>https://matasukef.github.io/papers/</id><updated>2018-06-17T00:00:00+09:00</updated><entry><title>Attention-based Multimodal Neural Machine Translation</title><link href="https://matasukef.github.io/papers/attention-based-multimodal-neural-machine-translation.html" rel="alternate"></link><published>2018-06-17T00:00:00+09:00</published><updated>2018-06-17T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-17:/papers/attention-based-multimodal-neural-machine-translation.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;入力画像の全体的な特徴と部分的な特徴を組み合わせることによって、画像を用いたMultimodal neural machine translationのSOTAを実現&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="MNMT"></category><category term="CV"></category></entry><entry><title>2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</title><link href="https://matasukef.github.io/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html" rel="alternate"></link><published>2018-06-12T00:00:00+09:00</published><updated>2018-06-12T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-12:/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;p&gt;A. Newell, K. Yang, and J. Deng. Stacked Hourglass Networks
for Human Pose Estimation&lt;/p&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Pose Estimation"></category><category term="Action Recognition"></category></entry><entry><title>Paper Survey</title><link href="https://matasukef.github.io/papers/paper-survey.html" rel="alternate"></link><published>2018-06-11T00:00:00+09:00</published><updated>2018-06-11T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-11:/papers/paper-survey.html</id><summary type="html">&lt;h1&gt;About&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/matasukef/papers"&gt;&lt;img alt="Build Status" src="https://travis-ci.org/matasukef/papers.svg?branch=master"&gt;&lt;/a&gt;
- NLPやCVに関連する論文の概要をまとめていきます。
- 読む予定の論文は&lt;a href="https://github.com/matasukef/papers/issues"&gt;Issue&lt;/a&gt;に上げます。
- 進捗を&lt;a href="https://github.com/matasukef/papers/projects"&gt;Projects&lt;/a&gt;にまとめています。
- 概要は&lt;a href="https://github.com/matasukef/papers/blob/master/Format.md"&gt;Format.md&lt;/a&gt;に基づいて作成します。&lt;/p&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/cv.html"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/nlp.html"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/ml.html"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Format&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="err"&gt;論文タイトル&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;YYYY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;MM&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;DD&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLP&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ACL2018&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WMT&lt;/span&gt;
&lt;span class="n"&gt;Author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Kosuke&lt;/span&gt; &lt;span class="n"&gt;Futamata&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hogehoge&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どんなもの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;先行研究と比べてどこがすごいの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;技術や手法の&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;キモ&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;はどこにある？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どうやって有効だと検証した？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;議論はあるか …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1&gt;About&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/matasukef/papers"&gt;&lt;img alt="Build Status" src="https://travis-ci.org/matasukef/papers.svg?branch=master"&gt;&lt;/a&gt;
- NLPやCVに関連する論文の概要をまとめていきます。
- 読む予定の論文は&lt;a href="https://github.com/matasukef/papers/issues"&gt;Issue&lt;/a&gt;に上げます。
- 進捗を&lt;a href="https://github.com/matasukef/papers/projects"&gt;Projects&lt;/a&gt;にまとめています。
- 概要は&lt;a href="https://github.com/matasukef/papers/blob/master/Format.md"&gt;Format.md&lt;/a&gt;に基づいて作成します。&lt;/p&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/cv.html"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/nlp.html"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/ml.html"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Format&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="err"&gt;論文タイトル&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;YYYY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;MM&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;DD&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLP&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ACL2018&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WMT&lt;/span&gt;
&lt;span class="n"&gt;Author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Kosuke&lt;/span&gt; &lt;span class="n"&gt;Futamata&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hogehoge&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どんなもの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;先行研究と比べてどこがすごいの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;技術や手法の&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;キモ&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;はどこにある？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どうやって有効だと検証した？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;議論はあるか？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;次に読むべき論文はあるか？&lt;/span&gt;

&lt;span class="err"&gt;###&lt;/span&gt; &lt;span class="err"&gt;論文情報・リンク&lt;/span&gt;

&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="err"&gt;著者，&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;タイトル，&amp;quot;&lt;/span&gt; &lt;span class="err"&gt;ジャーナル名，&lt;/span&gt;&lt;span class="n"&gt;voluem&lt;/span&gt;&lt;span class="err"&gt;，&lt;/span&gt;&lt;span class="n"&gt;no&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;，ページ，年&lt;/span&gt;&lt;span class="o"&gt;](&lt;/span&gt;&lt;span class="err"&gt;論文リンク&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link href="https://matasukef.github.io/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html" rel="alternate"></link><published>2018-06-11T00:00:00+09:00</published><updated>2018-06-11T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-11:/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;https://github.com/kelvinxu/arctic-captions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="Attention"></category></entry><entry><title>Video Question Answering via Hierarchical Spatio-Temporal Attention Networks</title><link href="https://matasukef.github.io/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html" rel="alternate"></link><published>2018-05-15T00:00:00+09:00</published><updated>2018-05-15T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-05-15:/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;VQA(Visual Question Answering)の一種で画像ではなくビデオを用いて、質問に対する回答を導出する研究。&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;従来研究では画像を主な&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;spatio-temporal attention network learning framework&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;-&lt;a href="https://www.ijcai.org/proceedings/2017/0492.pdf"&gt;Zhou Zhao, Qifan Yang, Deng Cai, Xiaofei He and Yueting Zhuang. Video Question Answering via Hierarchical Spatio-Temporal Attention Networks. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and
Aaron Courville. Describing videos by exploiting temporal structure. In ICCV, pages 4507–4515, 2015.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="VQA"></category><category term="JICAI-17"></category></entry></feed>