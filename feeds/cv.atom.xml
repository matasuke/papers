<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Paper Survey - CV</title><link href="https://matasukef.github.io/papers/" rel="alternate"></link><link href="https://matasukef.github.io/papers/feeds/cv.atom.xml" rel="self"></link><id>https://matasukef.github.io/papers/</id><updated>2018-07-23T00:00:00+09:00</updated><entry><title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title><link href="https://matasukef.github.io/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html" rel="alternate"></link><published>2018-07-23T00:00:00+09:00</published><updated>2018-07-23T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-07-23:/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;物体検出によるbottom-up attentionと重み付き平均を用いたtop-down attentionの両方を組み合わせることにより，Image CaptioningとVisual Question Answeringの両方のタスクにおいてSOTAを達成．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;従来のImage captioningやVideo Question Answeringのタスクではほとんどの場合，逐次生成されるキャプションの結果や質問と画像のpixel wise feature vectorによる重み付き平均によるtop-down型のvisual attentionを用いる．
一方で本研究では，画像のpixel wise feature vectoreではなく，Faster R-CNNなどの物体検出アルゴリズムを用いたbottom-up attentionの出力結果に対してtop-down attentionを適用している．&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;bottom-up attentionの出力結果をtop-down attentionに適用している．
物体検出アルゴリズムとして知られるFaster R-CNNの出力結果である部分画像に対してmean-pooled convolutionを適用したfeature vectoresに対してattentionを貼る．
さらに，これらfeature vectoresの平均を取ったのをNetworkの入力として用いる．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure2.png"&gt;&lt;/p&gt;
&lt;p&gt;bottom-up attentionを用いる以外はImage Captioning及びVisual Question AnsweringのNetworksに変わった構造は見られない．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure3.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure4.png"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visual Genome&lt;/li&gt;
&lt;li&gt;MSCOCO&lt;/li&gt;
&lt;li&gt;VQA v2.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;比較対象&lt;/h3&gt;
&lt;h3&gt;Image Captioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SCST(SOTAだったモデル)&lt;/li&gt;
&lt;li&gt;ResNet(提案手法のbottom-up attentionをResNetに置換)&lt;/li&gt;
&lt;li&gt;up-down(bottom-up and top-down approach)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ResNetはvisual attentionにResNet101を用いて，最終層のconv layerの出力を10*10にリサイズ．
通常のvisual attentionと同様に，pixel-wiseのfeature vectorに対してattentionを貼る．&lt;/p&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLEU, METEOR, ROUGE-L, CIEDErは高いほど良い，&lt;/li&gt;
&lt;li&gt;SPICEは小さいほど良い．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure5.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image6" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure6.png"&gt;&lt;/p&gt;
&lt;p&gt;Image Captioningのタスクでは全ての評価指標において，現SOTAのモデルを上回った．
VQAのタスクでは，2017 VQA challengeに投稿された全てのモデルを上回る正解率であった．&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure7.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure8.png"&gt;&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1708.02711.pdf"&gt;'Damien Teney, Peter Anderson, Xiaodong He, Anton van den Hengel', "Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",'2017 VQA Challenge'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1506.01497.pdf"&gt;'S. Ren, K. He, R. Girshick, and J. Sun', 'Faster R-CNN: Towards real-time object detection with region proposal networks', 'NIPS 2015'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1704.03162.pdf"&gt;'V. Kazemi and A. Elqursh', 'Show, ask, attend, and answer: A
strong baseline for visual question answering.'&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.panderson.me/images/1707.07998-up-down.pdf"&gt;"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang"，"Bottom-Up and Top-Down Attention for Image Captioning
and Visual Question Answering"，"CVPR 2018"，2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="VQA"></category><category term="CVPR2018"></category></entry><entry><title>SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text</title><link href="https://matasukef.github.io/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html" rel="alternate"></link><published>2018-07-02T00:00:00+09:00</published><updated>2018-07-02T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-07-02:/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;画像からスタイルを適用したキャプションをUnpaired imagesから生成する研究．
Term Generatorを用いて画像に関連する単語を生成して，生成された単語列からLanguage Generatornを用いてスタイルを適用したキャプションを生成する．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;画像とキャプションのPaired dataを用いず，キャプションデータのみを用いてスタイルを適用したキャプションを画像から生成する手法を提案.&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;Term Generator と Language Generatorの階層的構造によってスタイルを適用したキャプションを生成．
Term Generatorの学習時のみ画像とキャプションから構成されるPaired Dataを用いる．
Language Generatorの学習には，スタイルが適用されたキャプションのみを用いる．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle2.png"&gt;&lt;/p&gt;
&lt;h3&gt;Term Generator&lt;/h3&gt;
&lt;p&gt;意味論的に重要であると考えられる単語の羅列を画像から生成する．
単語列の生成には，Inception-v3とGRUによるEncoder-Decoder構造を取っている．
これは元祖Image caption generatorであるShow and tellと同様のネットワーク構造である．&lt;/p&gt;
&lt;p&gt;また，教師データにはキャプションから前置詞や助詞などの機能語を取り除いたものを用いる
lemmatizationやPOSなどの処理も施す．
動詞は画像キャプション生成に重要なワードだが，スタイルを持つためそのまま利用せず、FrameNetを用いて上位概念の単語に置換する．&lt;/p&gt;
&lt;h3&gt;Language Generator&lt;/h3&gt;
&lt;p&gt;Term Generatorにより生成された単語の羅列からスタイルを適用したキャプションを生成する．
Bi-directionalのGRUにAttentionを適用したEncoder-Decoder構造を取っている．&lt;/p&gt;
&lt;p&gt;教師データには単語の羅列と対応するキャプションが必要であるため，上述の手法によりキャプションから単語の羅列へのマッピングを作成する．
しかしStyled textを用いて作成された単語の羅列のみを用いてLanguage Generatorを学習させた場合，Paired dataから学習されたTerm Generatorによって出力される単語羅列を分布が異なってしまう(Paired dataの単語羅列がStyledtextの単語羅列に十分に現れるとは想定されないため)
そこでPaired data内におけるdescriptive CaptionとStyled textの両方を用いてLanguage Generatorを学習させる．
指定したスタイルのキャプション(Descriptive or Styled)を生成するため，入力される単語羅列の先頭に&lt;Descriptive&gt;,&lt;Styled&gt;などのタグを追加して学習させる(multilingual NMTでよく使われる手法らしい)．&lt;/p&gt;
&lt;p&gt;Paired Dataによって学習されたTerm Generatorと組み合わせることによって，様々なスタイルのキャプションを生成することが可能になる，&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MSCOCO&lt;/li&gt;
&lt;li&gt;bookcorpusに含まれる恋愛小説&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;比較対象&lt;/h3&gt;
&lt;h3&gt;Descriptive Caption&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CNN+RNN+coco(Show and tellのモデル)&lt;/li&gt;
&lt;li&gt;StyleNet(Factored LSTMを用いてDescriptive Captionを生成)&lt;/li&gt;
&lt;li&gt;SemStyle-cocoonly(提案手法でLanguage Generator部分をDescriptive captionのみで学習)&lt;/li&gt;
&lt;li&gt;SemStyle-coco(提案手法で単語羅列の先頭に&lt;Descriptive&gt;を付与したもの)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Styled caption&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;StyleNet(Factored LSTMを用いてStyled Captionを生成)&lt;/li&gt;
&lt;li&gt;TermRetrieval(Term Generatorによって生成された単語羅列を用いてStyled textのコーパスから文章を引っ張ってきたもの)&lt;/li&gt;
&lt;li&gt;neural-storyteller&lt;/li&gt;
&lt;li&gt;JointEmbedding&lt;/li&gt;
&lt;li&gt;SemStyle(提案手法)&lt;/li&gt;
&lt;li&gt;SemStyle-unordered(提案手法でTerm Generatorによって生成される単語羅列のランダムにしたもの)&lt;/li&gt;
&lt;li&gt;SemStyle-words(提案手法でTerm Generatorによって生成される単語羅列にlemmatizationやPos, FrameNetなどを適用しない)&lt;/li&gt;
&lt;li&gt;SemStyle-lempos(提案手法でTerm Generatorによって生成される単語羅列にFrameNetを適用しない)&lt;/li&gt;
&lt;li&gt;SemStyle-romanly(提案手法でTerm Generatorをbookcorpusの恋愛小説のみで学習)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLEU, METEOR, CIEDErは高いほど良い，&lt;/li&gt;
&lt;li&gt;SPICE, CLF, LM, GRULMは小さいほど良い．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image3" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle3.png"&gt;
&lt;img alt="image4" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle4.png"&gt;&lt;/p&gt;
&lt;h4&gt;human evaluation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;(a) 生成されたキャプションがどれほど画像の状況を言い表しているか．&lt;/li&gt;
&lt;li&gt;(b) 生成されたキャプションがUnrelated, Descriptive, Styledのどれに属するか.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image6" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle6.png"&gt;&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(a), (b),(c), (d)は正しく生成されたキャプション．&lt;/li&gt;
&lt;li&gt;(e), (f)は生成に失敗したキャプション&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle5.png"&gt;&lt;/p&gt;
&lt;p&gt;生成に成功したキャプションでは，Storyの方がDescriptiveより印象深い単語を使用していることが読み取れる．
また，過去形や定冠詞の使用，一人称視点からの叙述が見られる．&lt;/p&gt;
&lt;p&gt;失敗例は文法的には正しいが，常識に反するものが多く見られる．&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/Q/Q17/Q17-1024.pdf"&gt;'Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun,Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas,Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean', "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",'EMNLP 2017'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/Generating-Attractive-Visual-Captions-with-Styles.pdf"&gt;'Chuang Gan, Zhe Gan, Xiaodong He,Jianfeng Gao,Li Deng', 'StyleNet: Generating Attractive Visual Captions with Styles', 'CVPR 2017'&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1805.07030"&gt;"Alexander Mathews, Lexing Xie, Xuming He"，"SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text"，"CVPR 2018"，2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="CVPR2018"></category></entry><entry><title>2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</title><link href="https://matasukef.github.io/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html" rel="alternate"></link><published>2018-06-12T00:00:00+09:00</published><updated>2018-06-12T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-12:/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;p&gt;A. Newell, K. Yang, and J. Deng. Stacked Hourglass Networks
for Human Pose Estimation&lt;/p&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Pose Estimation"></category><category term="Action Recognition"></category></entry><entry><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link href="https://matasukef.github.io/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html" rel="alternate"></link><published>2018-06-11T00:00:00+09:00</published><updated>2018-06-11T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-11:/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;https://github.com/kelvinxu/arctic-captions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="Attention"></category></entry><entry><title>Video Question Answering via Hierarchical Spatio-Temporal Attention Networks</title><link href="https://matasukef.github.io/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html" rel="alternate"></link><published>2018-05-15T00:00:00+09:00</published><updated>2018-05-15T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-05-15:/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;動画とそれに関連する質問に回答するVQA(Video Question Answering)の研究．
質問に関連する重要な動画フレームの連続を学習するHierachical Spatio-Temporal Attention Networksを提案．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="{filename}/content/images/cv/VQA/image1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;先行研究では画像と質問文のペアから回答を生成するVisual Question Answeringの研究が主であるが，本研究では動画と質問文のペアから回答を生成する手法を提案．
動画フレームの連続性を考慮するために，Hierachical Spatio-Temporal Attention Networksを提案している．&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;spatio-temporal attention network learning framework&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="{filename}/content/images/cv/VQA/image2.png"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;-&lt;a href="https://www.ijcai.org/proceedings/2017/0492.pdf"&gt;Zhou Zhao, Qifan Yang, Deng Cai, Xiaofei He and Yueting Zhuang. Video Question Answering via Hierarchical Spatio-Temporal Attention Networks. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and
Aaron Courville. Describing videos by exploiting temporal structure. In ICCV, pages 4507–4515, 2015.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="VQA"></category><category term="JICAI-17"></category></entry></feed>