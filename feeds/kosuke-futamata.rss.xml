<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Paper Survey - Kosuke Futamata</title><link>https://matasukef.github.io/papers/</link><description>NLPやCVの論文についてまとめていきます</description><lastBuildDate>Thu, 26 Jul 2018 00:00:00 +0900</lastBuildDate><item><title>Rich Image Captioning in the Wild</title><link>https://matasukef.github.io/papers/rich-image-captioning-in-the-wild.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Thu, 26 Jul 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-07-26:/papers/rich-image-captioning-in-the-wild.html</guid><category>IDG</category><category>CVPR2018</category></item><item><title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title><link>https://matasukef.github.io/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Mon, 23 Jul 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-07-23:/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html</guid><category>IDG</category><category>VQA</category><category>CVPR2018</category></item><item><title>NICT-NAIST System for WMT17 Multimodal Translation Task</title><link>https://matasukef.github.io/papers/nict-naist-system-for-wmt17-multimodal-translation-task.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Tue, 03 Jul 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-07-03:/papers/nict-naist-system-for-wmt17-multimodal-translation-task.html</guid><category>NMT</category></item><item><title>SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text</title><link>https://matasukef.github.io/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Mon, 02 Jul 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-07-02:/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html</guid><category>IDG</category><category>CVPR2018</category></item><item><title>StyleNet: Generating Attractive Visual Captions with Styles</title><link>https://matasukef.github.io/papers/stylenet-generating-attractive-visual-captions-with-styles.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Thu, 28 Jun 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-06-28:/papers/stylenet-generating-attractive-visual-captions-with-styles.html</guid><category>IDA</category><category>CVPR2016</category></item><item><title>Attention-based Multimodal Neural Machine Translation</title><link>https://matasukef.github.io/papers/attention-based-multimodal-neural-machine-translation.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Sun, 17 Jun 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-06-17:/papers/attention-based-multimodal-neural-machine-translation.html</guid><category>MNMT</category><category>CV</category></item><item><title>2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</title><link>https://matasukef.github.io/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Tue, 12 Jun 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-06-12:/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html</guid><category>Pose Estimation</category><category>Action Recognition</category></item><item><title>Paper Survey</title><link>https://matasukef.github.io/papers/paper-survey.html</link><description>&lt;h1&gt;About&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;NLPやCVに関連する論文の概要をまとめていきます。&lt;/li&gt;
&lt;li&gt;読む予定の論文は&lt;a href="https://github.com/matasukef/papers/issues"&gt;Issue&lt;/a&gt;に上げます。&lt;/li&gt;
&lt;li&gt;進捗を&lt;a href="https://github.com/matasukef/papers/projects"&gt;Projects&lt;/a&gt;にまとめています。&lt;/li&gt;
&lt;li&gt;概要は&lt;a href="https://github.com/matasukef/papers/blob/master/Format.md"&gt;Format.md&lt;/a&gt;に基づいて作成します。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/cv.html"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/nlp.html"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/ml.html"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Format&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="err"&gt;論文タイトル&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;YYYY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;MM&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;DD&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLP&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ACL2018&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WMT&lt;/span&gt;
&lt;span class="n"&gt;Author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Kosuke&lt;/span&gt; &lt;span class="n"&gt;Futamata&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hogehoge&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どんなもの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;先行研究と比べてどこがすごいの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;技術や手法の&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;キモ&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;はどこにある？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どうやって有効だと検証した？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;議論はあるか …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Mon, 11 Jun 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-06-11:/papers/paper-survey.html</guid></item><item><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link>https://matasukef.github.io/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Mon, 11 Jun 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-06-11:/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html</guid><category>IDG</category><category>Attention</category></item><item><title>Video Question Answering via Hierarchical Spatio-Temporal Attention Networks</title><link>https://matasukef.github.io/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kosuke Futamata</dc:creator><pubDate>Tue, 15 May 2018 00:00:00 +0900</pubDate><guid isPermaLink="false">tag:matasukef.github.io,2018-05-15:/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html</guid><category>VQA</category><category>JICAI-17</category></item></channel></rss>