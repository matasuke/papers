<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Paper Survey - CV, NLP</title><link href="https://matasukef.github.io/papers/" rel="alternate"></link><link href="https://matasukef.github.io/papers/feeds/cv-nlp.atom.xml" rel="self"></link><id>https://matasukef.github.io/papers/</id><updated>2018-06-28T00:00:00+09:00</updated><entry><title>StyleNet: Generating Attractive Visual Captions with Styles</title><link href="https://matasukef.github.io/papers/stylenet-generating-attractive-visual-captions-with-styles.html" rel="alternate"></link><published>2018-06-28T00:00:00+09:00</published><updated>2018-06-28T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-28:/papers/stylenet-generating-attractive-visual-captions-with-styles.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;image captioningに関する研究
既存の研究はpaired dataを用いてfactualなcaption(画像の状況を客観的に記述したもの)を生成する手法が主流であったが、
本研究ではスタイルを適用したcaption(romantic, humorous)を生成している。&lt;/p&gt;
&lt;p&gt;以下の図にスタイルを適用したキャプションの例を示す。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fは特定スタイルを含まない客観的に画像を述べたキャプション&lt;/li&gt;
&lt;li&gt;Rは恋愛小説のスタイルを含むキャプション&lt;/li&gt;
&lt;li&gt;Hはユーモアのスタイルを含むキャプション&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img width="1271" alt="{filename}/images/cv/stylenet/factored_lstm2.png" src="stylenet/stylenet2.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;先行研究ではpaired dataによるfactualなcaption生成が主流であったが、
本研究ではMSCOCOなどのpaired dataと適用したいスタイルを含むunpairedなdata(文章の集合)を用いることによって、
スタイルを適用したキャプションを生成することを可能にした。&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;基本のモデルはEncoer側にCNN, Decoder側にRNNを用いたseq2seqだが、decoderのRNNにLSTM cellを３つに分解したFactored LSTMを使用。
Factored LSTM cellは従来のLSTM cellのパラメーターを分解($W_x = U_xS_xV_x$)したものである。
したがって、Factored LSTMのcellは以下のようになる。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i_t = sigmoid(U_{ix}S_{ix}V_{ix}x_t + W_{ih}h_{t-1}) \\
f_t = sigmoid(U_{fx}S_{fx}V_{fx}x_t + W_{fh}h_{t-1}) \\
o_t = sigmoid(U_{ox}S_{ox}V_{ox}x_t + W_{oh}h_{t-1}) \\
\bar{c}_t = tanh(U_{cx}S_{cx}V_{cx}x_t + W_{ch}h_{t-1}) \\
c_t = f_t \odot c_{t-1} + i_t \odot \bar{c}_t \\
c_t = o_t \odot c_t
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;t番目のステップのtokenに対してのみ、分解した全パラメータを用い、t-1番目の状態である$h_t-1$に対しては$W_h$を用いる。
&lt;img width="1271" alt="{filename}/images/cv/stylenet/factored_lstm.png" src="stylenet/stylenet1.png"&gt;&lt;/p&gt;
&lt;p&gt;また、学習時にはFacturalなcaption生成用のDecoderとスタイルを適用したcaption生成用のDecoderを1 poech毎に切り替えたmulti task learningを行う。
Factored LSTMのパラメータSのみを入れ替えつつfacturel captionとstyled captionの生成を行うため、contextに関する情報がU, Vに学習され、Sにstyleが学習されると想定される。&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;Facored LSTMによってパラメータ$S_x$に対してのみスタイルを適用するメカニズムがいまいち謎。
全体的にお気持論文な傾向がある。。。
CNNベースでスタイルを適用したcaption生成が可能なら、RNNを用いて文章のStyle transferに応用できるのでは？&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/research/uploads/prod/2017/06/Generating-Attractive-Visual-Captions-with-Styles.pdf"&gt;Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng, StyleNet: Generating Attractive Visual Captions with Styles, CVPR, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDA"></category><category term="CVPR2016"></category></entry></feed>