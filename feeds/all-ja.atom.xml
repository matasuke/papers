<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Paper Survey</title><link href="https://matasukef.github.io/papers/" rel="alternate"></link><link href="https://matasukef.github.io/papers/feeds/all-ja.atom.xml" rel="self"></link><id>https://matasukef.github.io/papers/</id><updated>2018-08-02T00:00:00+09:00</updated><entry><title>Stacked Attention Networks for Image Question Answering</title><link href="https://matasukef.github.io/papers/stacked-attention-networks-for-image-question-answering.html" rel="alternate"></link><published>2018-08-02T00:00:00+09:00</published><updated>2018-08-02T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-08-02:/papers/stacked-attention-networks-for-image-question-answering.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;Visual Question Answerigに用いられるStacked Attention networkを提案。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure1.png" alt="image1" title="image1" width="100" height="100"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;本論文で提案しているStacked Attention Mechanismを用いることで、VQAのタスクにおいてSOTAを達成。
また、本論文の投稿時にはAttention mechanismをVQAのタスクに適用したものとしては初。&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;Stacked attention mechanismによって画像内の物体と質問文のアライメントを行う。
Attentionをスタック構造にすることによって画像内の物体への注意がよりシャープになるため、より特定の物体に注意を向けることが容易になっている。&lt;/p&gt;
&lt;p&gt;ネットワークの構造は主に３つに分解される。
1つ目はImage model, 2つ目はQuestion Model,そして3つ目がStacked Attention Networksである。
Image modelはVGGNetを用いて画像の特徴量を抽出するのに用いられる。
Question modelはLSTMまたはCNNを用いてQuestionをEncodeする。&lt;/p&gt;
&lt;p&gt;Stacked Sttention Networksは画像特徴量と質問文の特徴量を用いて画像に対するAttentionを得る。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure3.png" alt="image1" title="image1" width="100" height="50"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure4.png" alt="image1" title="image1" width="200" height="50"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure5.png" alt="image1" title="image1" width="200" height="50"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure6.png" alt="image1" title="image1" width="200" height="50"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure7.png" alt="image1" title="image1" width="200" height="50"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DAQUAR-ALL&lt;/li&gt;
&lt;li&gt;DAQUAR-REDUCED&lt;/li&gt;
&lt;li&gt;COCO-QA&lt;/li&gt;
&lt;li&gt;VQA&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure8.png" alt="image1" title="image1" width="200" height="200"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure9.png" alt="image1" title="image1" width="200" height="200"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure10.png" alt="image1" title="image1" width="200" height="200"&gt;
&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure11.png" alt="image1" title="image1" width="200" height="200"&gt;&lt;/p&gt;
&lt;p&gt;人間を除く全ての比較対象と比べ提案手法が一番精度が良かった。&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;image captioningやmultimoda NMTの領域にstacked attentionを適用するのも良さそう。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure14.png" alt="image1" title="image1" width="200" height="200"&gt;&lt;/p&gt;
&lt;p&gt;解答に失敗した例を見るとほとんどの場合、質問文に対するvisual attentionの画像内領域は正しい。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://matasukef.github.io/papers/images/cv/stacked-attention-network/figure15.png" alt="image1" title="image1" width="200" height="200"&gt;&lt;/p&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1511.02274.pdf"&gt;"Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola"，"Stacked Attention Networks for Image Question Answering"，"CoRR"，2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="VQA"></category></entry><entry><title>Rich Image Captioning in the Wild</title><link href="https://matasukef.github.io/papers/rich-image-captioning-in-the-wild.html" rel="alternate"></link><published>2018-07-26T00:00:00+09:00</published><updated>2018-07-26T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-07-26:/papers/rich-image-captioning-in-the-wild.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;画像に映る様々なvisual conceptsを含む表現豊かなキャプションを生成する研究。
近年のimage captioningに関する研究はend-to-endが主であるが、この論文ではいくつかのモジュールから構成されるcompositional modelである。
人手の評価を用いてcompositional modelにおいてSOTAを達成。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://matasukef.github.io/papers/images/cv/rich-image-captioning/figure1.png"&gt;&lt;/p&gt;
&lt;p&gt;上記の画像の例では、物体研究を用いることでcelebritiesとlandmarksをキャプションの結果に反映させた例である。&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;近年のimage captioningに関する研究では主にend-to-endによるMSCOCO captioning datasetやfliker datasetのみを用いる傾向にある。
したがって、open-domainな画像に対して強健なモデルとは言い難い。
また、モデルの評価時にBleu, spice, Ciderなどの指標を用いるが、これらの指標は人手の評価との相関が低い。&lt;/p&gt;
&lt;p&gt;この研究では、画像に描画されるvisual conceptsを生成されるキャプションに組み込むことによって表現豊かなcaptioningを行う。
End-to-Endモデルとの比較は行われていないものの、従来のcompositional modelよりキャプションの精度が良かった。&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;様々なモジュールの組み合わせることによってimage captioningを行っている。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/rich-image-captioning/figure2.png"&gt;&lt;/p&gt;
&lt;p&gt;CNNベースの分類器を用いて物体の検出(Visual concepts, Celebrity, Landmark)を行い、maximum entropy language model(MELM)によりキャプションを生成する。
また、Deep multimodal similarity model(DMSM)により、input imageのCNNを適用した後の特徴量とMELMにより生成されたキャプションを同一の意味空間上に落とし込む。
DMSMにより、画像とキャプションのcosin距離を計算し、キャプションの信頼性のscoreとして用いる。
Confidence modelは単純なロジスティク回帰モデルである。
DMSMのscore, MELMのscore, caption lengthなどを特徴量として用いる。
confidenc modelの確率が高いキャプションはそのまま出力し、低い場合は
"This image contains XXX, YYY, ZZZ"のよう物体検出の結果を出力する。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/rich-image-captioning/figure3.png"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MSCOCO&lt;/li&gt;
&lt;li&gt;MIT Dataset&lt;/li&gt;
&lt;li&gt;images on imstagram sampled randomly&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;比較対象&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fang et al(SOTAだったモデル)&lt;/li&gt;
&lt;li&gt;Basic(confidence model, 物体検出なし)&lt;/li&gt;
&lt;li&gt;Basic+Confi(物体検出なし)&lt;/li&gt;
&lt;li&gt;Full(提案手法)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;p&gt;人手の評価を行った。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/rich-image-captioning/figure4.png"&gt;&lt;/p&gt;
&lt;p&gt;compositional modelの従来のSOTAであったモデルよりキャプションの精度が良かった。&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;Bleuなどの評価指標を用いた実験を行っていないため、他のEnd-to-Endモデルとの比較ができない。
instagram上のopen-domainな画像に対してはExcellent及びGoodと判断されたキャプションが全体の50%であった。&lt;/p&gt;
&lt;p&gt;Confidence scoreが低い時に物体検出の結果を出力するのはどうかと思った。
物体検出の結果を出力すれば、Goodの割合が必然的に増加するのではないかと考えられる。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/rich-image-captioning/figure5.png"&gt;&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1411.4952.pdf"&gt;'Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig', "From Captions to Visual Concepts and Back",'CVPR 2015',2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1603.09016.pdf"&gt;"Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, Chris Sienkiewicz"，"Rich Image Captioning in the Wild"，"CVPR 2018"，2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="CVPR2018"></category></entry><entry><title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title><link href="https://matasukef.github.io/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html" rel="alternate"></link><published>2018-07-23T00:00:00+09:00</published><updated>2018-07-23T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-07-23:/papers/bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;物体検出によるbottom-up attentionと重み付き平均を用いたtop-down attentionの両方を組み合わせることにより，Image CaptioningとVisual Question Answeringの両方のタスクにおいてSOTAを達成．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;従来のImage captioningやVideo Question Answeringのタスクではほとんどの場合，逐次生成されるキャプションの結果や質問と画像のpixel wise feature vectorによる重み付き平均によるtop-down型のvisual attentionを用いる．
一方で本研究では，画像のpixel wise feature vectoreではなく，Faster R-CNNなどの物体検出アルゴリズムを用いたbottom-up attentionの出力結果に対してtop-down attentionを適用している．&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;bottom-up attentionの出力結果をtop-down attentionに適用している．
物体検出アルゴリズムとして知られるFaster R-CNNの出力結果である部分画像に対してmean-pooled convolutionを適用したfeature vectoresに対してattentionを貼る．
さらに，これらfeature vectoresの平均を取ったのをNetworkの入力として用いる．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure2.png"&gt;&lt;/p&gt;
&lt;p&gt;bottom-up attentionを用いる以外はImage Captioning及びVisual Question AnsweringのNetworksに変わった構造は見られない．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure3.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure4.png"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visual Genome&lt;/li&gt;
&lt;li&gt;MSCOCO&lt;/li&gt;
&lt;li&gt;VQA v2.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;比較対象&lt;/h3&gt;
&lt;h3&gt;Image Captioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SCST(SOTAだったモデル)&lt;/li&gt;
&lt;li&gt;ResNet(提案手法のbottom-up attentionをResNetに置換)&lt;/li&gt;
&lt;li&gt;up-down(bottom-up and top-down approach)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ResNetはvisual attentionにResNet101を用いて，最終層のconv layerの出力を10*10にリサイズ．
通常のvisual attentionと同様に，pixel-wiseのfeature vectorに対してattentionを貼る．&lt;/p&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLEU, METEOR, ROUGE-L, CIEDErは高いほど良い，&lt;/li&gt;
&lt;li&gt;SPICEは小さいほど良い．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure5.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image6" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure6.png"&gt;&lt;/p&gt;
&lt;p&gt;Image Captioningのタスクでは全ての評価指標において，現SOTAのモデルを上回った．
VQAのタスクでは，2017 VQA challengeに投稿された全てのモデルを上回る正解率であった．&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure7.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/bottom-up_and_top-down/figure8.png"&gt;&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1708.02711.pdf"&gt;'Damien Teney, Peter Anderson, Xiaodong He, Anton van den Hengel', "Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",'2017 VQA Challenge'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1506.01497.pdf"&gt;'S. Ren, K. He, R. Girshick, and J. Sun', 'Faster R-CNN: Towards real-time object detection with region proposal networks', 'NIPS 2015'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1704.03162.pdf"&gt;'V. Kazemi and A. Elqursh', 'Show, ask, attend, and answer: A
strong baseline for visual question answering.'&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.panderson.me/images/1707.07998-up-down.pdf"&gt;"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang"，"Bottom-Up and Top-Down Attention for Image Captioning
and Visual Question Answering"，"CVPR 2018"，2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="VQA"></category><category term="CVPR2018"></category></entry><entry><title>SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text</title><link href="https://matasukef.github.io/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html" rel="alternate"></link><published>2018-07-02T00:00:00+09:00</published><updated>2018-07-02T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-07-02:/papers/semstyle-learning-to-generate-stylised-image-captions-using-unaligned-text.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;画像からスタイルを適用したキャプションをUnpaired imagesから生成する研究．
Term Generatorを用いて画像に関連する単語を生成して，生成された単語列からLanguage Generatornを用いてスタイルを適用したキャプションを生成する．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;画像とキャプションのPaired dataを用いず，キャプションデータのみを用いてスタイルを適用したキャプションを画像から生成する手法を提案.&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;Term Generator と Language Generatorの階層的構造によってスタイルを適用したキャプションを生成．
Term Generatorの学習時のみ画像とキャプションから構成されるPaired Dataを用いる．
Language Generatorの学習には，スタイルが適用されたキャプションのみを用いる．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle2.png"&gt;&lt;/p&gt;
&lt;h3&gt;Term Generator&lt;/h3&gt;
&lt;p&gt;意味論的に重要であると考えられる単語の羅列を画像から生成する．
単語列の生成には，Inception-v3とGRUによるEncoder-Decoder構造を取っている．
これは元祖Image caption generatorであるShow and tellと同様のネットワーク構造である．&lt;/p&gt;
&lt;p&gt;また，教師データにはキャプションから前置詞や助詞などの機能語を取り除いたものを用いる
lemmatizationやPOSなどの処理も施す．
動詞は画像キャプション生成に重要なワードだが，スタイルを持つためそのまま利用せず、FrameNetを用いて上位概念の単語に置換する．&lt;/p&gt;
&lt;h3&gt;Language Generator&lt;/h3&gt;
&lt;p&gt;Term Generatorにより生成された単語の羅列からスタイルを適用したキャプションを生成する．
Bi-directionalのGRUにAttentionを適用したEncoder-Decoder構造を取っている．&lt;/p&gt;
&lt;p&gt;教師データには単語の羅列と対応するキャプションが必要であるため，上述の手法によりキャプションから単語の羅列へのマッピングを作成する．
しかしStyled textを用いて作成された単語の羅列のみを用いてLanguage Generatorを学習させた場合，Paired dataから学習されたTerm Generatorによって出力される単語羅列を分布が異なってしまう(Paired dataの単語羅列がStyledtextの単語羅列に十分に現れるとは想定されないため)
そこでPaired data内におけるdescriptive CaptionとStyled textの両方を用いてLanguage Generatorを学習させる．
指定したスタイルのキャプション(Descriptive or Styled)を生成するため，入力される単語羅列の先頭に&lt;Descriptive&gt;,&lt;Styled&gt;などのタグを追加して学習させる(multilingual NMTでよく使われる手法らしい)．&lt;/p&gt;
&lt;p&gt;Paired Dataによって学習されたTerm Generatorと組み合わせることによって，様々なスタイルのキャプションを生成することが可能になる，&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h3&gt;データセット&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MSCOCO&lt;/li&gt;
&lt;li&gt;bookcorpusに含まれる恋愛小説&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;比較対象&lt;/h3&gt;
&lt;h3&gt;Descriptive Caption&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CNN+RNN+coco(Show and tellのモデル)&lt;/li&gt;
&lt;li&gt;StyleNet(Factored LSTMを用いてDescriptive Captionを生成)&lt;/li&gt;
&lt;li&gt;SemStyle-cocoonly(提案手法でLanguage Generator部分をDescriptive captionのみで学習)&lt;/li&gt;
&lt;li&gt;SemStyle-coco(提案手法で単語羅列の先頭に&lt;Descriptive&gt;を付与したもの)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Styled caption&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;StyleNet(Factored LSTMを用いてStyled Captionを生成)&lt;/li&gt;
&lt;li&gt;TermRetrieval(Term Generatorによって生成された単語羅列を用いてStyled textのコーパスから文章を引っ張ってきたもの)&lt;/li&gt;
&lt;li&gt;neural-storyteller&lt;/li&gt;
&lt;li&gt;JointEmbedding&lt;/li&gt;
&lt;li&gt;SemStyle(提案手法)&lt;/li&gt;
&lt;li&gt;SemStyle-unordered(提案手法でTerm Generatorによって生成される単語羅列のランダムにしたもの)&lt;/li&gt;
&lt;li&gt;SemStyle-words(提案手法でTerm Generatorによって生成される単語羅列にlemmatizationやPos, FrameNetなどを適用しない)&lt;/li&gt;
&lt;li&gt;SemStyle-lempos(提案手法でTerm Generatorによって生成される単語羅列にFrameNetを適用しない)&lt;/li&gt;
&lt;li&gt;SemStyle-romanly(提案手法でTerm Generatorをbookcorpusの恋愛小説のみで学習)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;評価&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLEU, METEOR, CIEDErは高いほど良い，&lt;/li&gt;
&lt;li&gt;SPICE, CLF, LM, GRULMは小さいほど良い．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image3" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle3.png"&gt;
&lt;img alt="image4" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle4.png"&gt;&lt;/p&gt;
&lt;h4&gt;human evaluation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;(a) 生成されたキャプションがどれほど画像の状況を言い表しているか．&lt;/li&gt;
&lt;li&gt;(b) 生成されたキャプションがUnrelated, Descriptive, Styledのどれに属するか.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image6" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle6.png"&gt;&lt;/p&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;(a), (b),(c), (d)は正しく生成されたキャプション．&lt;/li&gt;
&lt;li&gt;(e), (f)は生成に失敗したキャプション&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="image5" src="https://matasukef.github.io/papers/images/cv/semstyle/semstyle5.png"&gt;&lt;/p&gt;
&lt;p&gt;生成に成功したキャプションでは，Storyの方がDescriptiveより印象深い単語を使用していることが読み取れる．
また，過去形や定冠詞の使用，一人称視点からの叙述が見られる．&lt;/p&gt;
&lt;p&gt;失敗例は文法的には正しいが，常識に反するものが多く見られる．&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/Q/Q17/Q17-1024.pdf"&gt;'Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun,Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas,Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean', "Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",'EMNLP 2017'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/Generating-Attractive-Visual-Captions-with-Styles.pdf"&gt;'Chuang Gan, Zhe Gan, Xiaodong He,Jianfeng Gao,Li Deng', 'StyleNet: Generating Attractive Visual Captions with Styles', 'CVPR 2017'&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1805.07030"&gt;"Alexander Mathews, Lexing Xie, Xuming He"，"SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text"，"CVPR 2018"，2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="CVPR2018"></category></entry><entry><title>StyleNet: Generating Attractive Visual Captions with Styles</title><link href="https://matasukef.github.io/papers/stylenet-generating-attractive-visual-captions-with-styles.html" rel="alternate"></link><published>2018-06-28T00:00:00+09:00</published><updated>2018-06-28T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-28:/papers/stylenet-generating-attractive-visual-captions-with-styles.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;image captioningに関する研究
既存の研究はpaired dataを用いてfactualなcaption(画像の状況を客観的に記述したもの)を生成する手法が主流であったが、
本研究ではスタイルを適用したcaption(romantic, humorous)を生成している。&lt;/p&gt;
&lt;p&gt;以下の図にスタイルを適用したキャプションの例を示す。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fは特定スタイルを含まない客観的に画像を述べたキャプション&lt;/li&gt;
&lt;li&gt;Rは恋愛小説のスタイルを含むキャプション&lt;/li&gt;
&lt;li&gt;Hはユーモアのスタイルを含むキャプション&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img width="1271" alt="stylenet1" src='https://matasukef.github.io/papers/images/cv/stylenet/stylenet2.png'&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;先行研究ではpaired dataによるfactualなcaption生成が主流であったが、
本研究ではMSCOCOなどのpaired dataと適用したいスタイルを含むunpairedなdata(文章の集合)を用いることによって、
スタイルを適用したキャプションを生成することを可能にした。&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;基本のモデルはEncoer側にCNN, Decoder側にRNNを用いたseq2seqだが、decoderのRNNにLSTM cellを３つに分解したFactored LSTMを使用。
Factored LSTM cellは従来のLSTM cellのパラメーターを分解(&lt;span class="math"&gt;\(W_x = U_xS_xV_x\)&lt;/span&gt;)したものである。
したがって、Factored LSTMのcellは以下のようになる。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$i_t = sigmoid(U_{ix}S_{ix}V_{ix}x_t + W_{ih}h_{t-1}) $ \\
$f_t = sigmoid(U_{fx}S_{fx}V_{fx}x_t + W_{fh}h_{t-1}) $ \\
$o_t = sigmoid(U_{ox}S_{ox}V_{ox}x_t + W_{oh}h_{t-1}) $ \\
$\bar{c}_t = tanh(U_{cx}S_{cx}V_{cx}x_t + W_{ch}h_{t-1}) $ \\
$c_t = f_t \odot c_{t-1} + i__t \odot \bar{c}t $ \\
$c_t = o_t \odot c_t $
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;t番目のステップのtokenに対してのみ、分解した全パラメータを用い、t-1番目の状態である&lt;span class="math"&gt;\(h_t-1\)&lt;/span&gt;に対しては&lt;span class="math"&gt;\(W_h\)&lt;/span&gt;を用いる。
&lt;img width="1271" alt="stylenet2" src='https://matasukef.github.io/papers/images/cv/stylenet/stylenet1.png'&gt;&lt;/p&gt;
&lt;p&gt;また、学習時にはFacturalなcaption生成用のDecoderとスタイルを適用したcaption生成用のDecoderを1 poech毎に切り替えたmulti task learningを行う。
Factored LSTMのパラメータSのみを入れ替えつつfacturel captionとstyled captionの生成を行うため、contextに関する情報がU, Vに学習され、Sにstyleが学習されると想定される。&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;p&gt;Facored LSTMによってパラメータ&lt;span class="math"&gt;\(S_x\)&lt;/span&gt;に対してのみスタイルを適用するメカニズムがいまいち謎。
全体的にお気持論文な傾向がある。。。
CNNベースでスタイルを適用したcaption生成が可能なら、RNNを用いて文章のStyle transferに応用できるのでは？&lt;/p&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/en-us/research/uploads/prod/2017/06/Generating-Attractive-Visual-Captions-with-Styles.pdf"&gt;Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng, StyleNet: Generating Attractive Visual Captions with Styles, CVPR, 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="IDA"></category><category term="CVPR2016"></category></entry><entry><title>Attention-based Multimodal Neural Machine Translation</title><link href="https://matasukef.github.io/papers/attention-based-multimodal-neural-machine-translation.html" rel="alternate"></link><published>2018-06-17T00:00:00+09:00</published><updated>2018-06-17T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-17:/papers/attention-based-multimodal-neural-machine-translation.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;入力画像の全体的な特徴と部分的な特徴を組み合わせることによって、画像を用いたMultimodal neural machine translationのSOTAを実現&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="MNMT"></category><category term="CV"></category></entry><entry><title>2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</title><link href="https://matasukef.github.io/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html" rel="alternate"></link><published>2018-06-12T00:00:00+09:00</published><updated>2018-06-12T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-12:/papers/2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;p&gt;A. Newell, K. Yang, and J. Deng. Stacked Hourglass Networks
for Human Pose Estimation&lt;/p&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Pose Estimation"></category><category term="Action Recognition"></category></entry><entry><title>Paper Survey</title><link href="https://matasukef.github.io/papers/paper-survey.html" rel="alternate"></link><published>2018-06-11T00:00:00+09:00</published><updated>2018-06-11T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-11:/papers/paper-survey.html</id><summary type="html">&lt;h1&gt;About&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;NLPやCVに関連する論文の概要をまとめていきます。&lt;/li&gt;
&lt;li&gt;読む予定の論文は&lt;a href="https://github.com/matasukef/papers/issues"&gt;Issue&lt;/a&gt;に上げます。&lt;/li&gt;
&lt;li&gt;進捗を&lt;a href="https://github.com/matasukef/papers/projects"&gt;Projects&lt;/a&gt;にまとめています。&lt;/li&gt;
&lt;li&gt;概要は&lt;a href="https://github.com/matasukef/papers/blob/master/Format.md"&gt;Format.md&lt;/a&gt;に基づいて作成します。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/cv.html"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/nlp.html"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/ml.html"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Format&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="err"&gt;論文タイトル&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;YYYY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;MM&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;DD&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLP&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ACL2018&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WMT&lt;/span&gt;
&lt;span class="n"&gt;Author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Kosuke&lt;/span&gt; &lt;span class="n"&gt;Futamata&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hogehoge&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どんなもの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;先行研究と比べてどこがすごいの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;技術や手法の&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;キモ&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;はどこにある？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どうやって有効だと検証した？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;議論はあるか …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1&gt;About&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;NLPやCVに関連する論文の概要をまとめていきます。&lt;/li&gt;
&lt;li&gt;読む予定の論文は&lt;a href="https://github.com/matasukef/papers/issues"&gt;Issue&lt;/a&gt;に上げます。&lt;/li&gt;
&lt;li&gt;進捗を&lt;a href="https://github.com/matasukef/papers/projects"&gt;Projects&lt;/a&gt;にまとめています。&lt;/li&gt;
&lt;li&gt;概要は&lt;a href="https://github.com/matasukef/papers/blob/master/Format.md"&gt;Format.md&lt;/a&gt;に基づいて作成します。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/cv.html"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/nlp.html"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matasukef.github.io/papers/category/ml.html"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Format&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="err"&gt;論文タイトル&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;YYYY&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;MM&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;DD&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;CV&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLP&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ACL2018&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WMT&lt;/span&gt;
&lt;span class="n"&gt;Author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Kosuke&lt;/span&gt; &lt;span class="n"&gt;Futamata&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hogehoge&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どんなもの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;先行研究と比べてどこがすごいの？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;技術や手法の&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;キモ&amp;quot;&lt;/span&gt;&lt;span class="err"&gt;はどこにある？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;どうやって有効だと検証した？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;議論はあるか？&lt;/span&gt;

&lt;span class="err"&gt;##&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="err"&gt;次に読むべき論文はあるか？&lt;/span&gt;

&lt;span class="err"&gt;###&lt;/span&gt; &lt;span class="err"&gt;論文情報・リンク&lt;/span&gt;

&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="err"&gt;著者，&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;タイトル，&amp;quot;&lt;/span&gt; &lt;span class="err"&gt;ジャーナル名，&lt;/span&gt;&lt;span class="n"&gt;voluem&lt;/span&gt;&lt;span class="err"&gt;，&lt;/span&gt;&lt;span class="n"&gt;no&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;，ページ，年&lt;/span&gt;&lt;span class="o"&gt;](&lt;/span&gt;&lt;span class="err"&gt;論文リンク&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><link href="https://matasukef.github.io/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html" rel="alternate"></link><published>2018-06-11T00:00:00+09:00</published><updated>2018-06-11T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-06-11:/papers/show-attend-and-tell-neural-image-caption-generation-with-visual-attention.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;https://github.com/kelvinxu/arctic-captions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="IDG"></category><category term="Attention"></category></entry><entry><title>Video Question Answering via Hierarchical Spatio-Temporal Attention Networks</title><link href="https://matasukef.github.io/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html" rel="alternate"></link><published>2018-05-15T00:00:00+09:00</published><updated>2018-05-15T00:00:00+09:00</updated><author><name>Kosuke Futamata</name></author><id>tag:matasukef.github.io,2018-05-15:/papers/video-question-answering-via-hierarchical-spatio-temporal-attention-networks.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. どんなもの？&lt;/h2&gt;
&lt;p&gt;VQA(Video Question Answering)の研究．
質問に関連する重要な動画フレームの連続を学習するHierachical Spatio-Temporal Attention Networksを提案．&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="{filename}/content/images/cv/VQA/image1.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. 先行研究と比べてどこがすごいの？&lt;/h2&gt;
&lt;p&gt;先行研究では画像と質問文のペアから回答を生成するVisual Question Answeringの研究が主であるが，本研究では動画と質問文のペアから回答を生成する手法を提案．
動画フレームの連続性を考慮するために，Hierachical Spatio-Temporal Attention Networksを提案している．&lt;/p&gt;
&lt;h2&gt;3. 技術や手法の"キモ"はどこにある？&lt;/h2&gt;
&lt;p&gt;spatio-temporal attention network learning framework&lt;/p&gt;
&lt;p&gt;&lt;img alt="image2" src="{filename}/content/images/cv/VQA/image2.png"&gt;&lt;/p&gt;
&lt;h2&gt;4. どうやって有効だと検証した？&lt;/h2&gt;
&lt;h2&gt;5. 議論はあるか？&lt;/h2&gt;
&lt;h2&gt;6. 次に読むべき論文はあるか？&lt;/h2&gt;
&lt;h3&gt;論文情報・リンク&lt;/h3&gt;
&lt;p&gt;-&lt;a href="https://www.ijcai.org/proceedings/2017/0492.pdf"&gt;Zhou Zhao, Qifan Yang, Deng Cai, Xiaofei He and Yueting Zhuang. Video Question Answering via Hierarchical Spatio-Temporal Attention Networks. Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, and
Aaron Courville. Describing videos by exploiting temporal structure. In ICCV, pages 4507–4515, 2015.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="論文リンク"&gt;著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="VQA"></category><category term="JICAI-17"></category></entry></feed>